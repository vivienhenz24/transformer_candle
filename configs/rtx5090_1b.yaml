# RTX 5090 32GB optimized 1.2B parameter training profile
# Optimized for 32GB VRAM with larger model and memory-efficient settings

model:
  hidden_size: 1280          # Increased: 1.2B param count
  intermediate_size: 5120    # Increased: 4x hidden_size
  num_layers: 16             # Increased: 1.2B param count
  num_attention_heads: 20    # Increased
  num_key_value_heads: 20    # Increased (use 8 for GQA to save memory if needed)
  max_position_embeddings: 512  # Reduced: shorter context for memory efficiency
  attn_dropout: null
  residual_dropout: 0.0
  rope_mode: null
  rope_theta: null

tokenizer:
  tokenizer_json: "/workspace/artifacts/tokenizer/rtx5090-1b/tokenizer.json"
  vocab: "/workspace/artifacts/tokenizer/rtx5090-1b/vocab.json"
  merges: "/workspace/artifacts/tokenizer/rtx5090-1b/merges.txt"
  special_tokens: "/workspace/runs/rtx5090-1b/special_tokens.txt"

data:
  train_shards: []  # Empty for streaming mode
  validation_shards: []  # Empty for streaming mode
  sequence_length: 512
  batch_size: 8              # Working: 8 with 8 grad_accum (effective batch=64)
  gradient_accumulation_steps: 8  # Working: 8 to maintain effective batch (64 samples)
  shuffle_buffer_size: 32768  # Working: Good balance
  num_workers: 6             # Working: Good balance
  cache_dir: "/workspace/cache/rtx5090-1b"

optimizer:
  algorithm: adam_w
  learning_rate: 1.8e-4      # Same: learning rate
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  epsilon: 1.0e-8

scheduler:
  strategy: cosine_with_warmup
  warmup_steps: 3000
  total_steps: 300000
  total_epochs: null
  min_lr: 1.8e-5
  max_lr: null

runtime:
  seed: 42
  precision: fp32            # FP32 for stability (working well)
  log_every_n_steps: 50
  checkpoint:
    directory: "/workspace/runs/rtx5090-1b/checkpoints"
    every_n_steps: 1500
    every_n_epochs: null
    max_keep: 5              # REDUCED: Keep fewer checkpoints to save disk
  evaluation:
    every_n_steps: 4500
    every_n_epochs: null
    max_batches: 16          # REDUCED: Fewer eval batches
    best:
      directory: "/workspace/runs/rtx5090-1b/best"
      max_keep: 2
  logging:
    enable_stdout: true
    tensorboard: "/workspace/runs/rtx5090-1b/tensorboard"
    tensorboard_flush_every_n: 100

# Memory Usage Estimate (RTX 5090 32GB):
# - Model parameters: ~5-6 GB (1.2B params in FP32)
# - Optimizer states: ~10-12 GB (AdamW has 2 states per param)
# - Gradients: ~5-6 GB
# - Activations (batch=8, seq=512): ~8-10 GB
# - Training overhead: ~2-3 GB
# Total: ~30-37 GB (working well at 37% usage!)
#
# Current Performance:
# - GPU Usage: 97% (excellent)
# - VRAM Usage: 37% (12GB out of 32GB)
# - Model Size: ~1.2B parameters
# - Training Speed: ~3K tokens/sec
#
# If OOM occurs:
# 1. Reduce batch_size to 4
# 2. Use GQA: set num_key_value_heads: 10 (saves ~1-2GB)
# 3. Enable gradient checkpointing (if implemented)
