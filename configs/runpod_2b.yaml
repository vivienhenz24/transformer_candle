# RunPod-targeted training configuration for the ~2B parameter model.
model:
  hidden_size: 2560
  intermediate_size: 10240
  num_layers: 24
  num_attention_heads: 40
  num_key_value_heads: 40
  max_position_embeddings: 4096
  attn_dropout: 0.0
  residual_dropout: 0.0
  rope_mode: on_the_fly
  rope_theta: null

tokenizer:
  tokenizer_json: "/workspace/artifacts/tokenizer/tokenizer.json"
  vocab: null
  merges: null
  special_tokens: "/workspace/artifacts/tokenizer/special_tokens.txt"

data:
  train_shards:
    - "/workspace/datasets/train/shard00.jsonl"
    - "/workspace/datasets/train/shard01.jsonl"
    - "/workspace/datasets/train/shard02.jsonl"
    - "/workspace/datasets/train/shard03.jsonl"
  validation_shards:
    - "/workspace/datasets/val/shard00.jsonl"
  batch_size: 2
  gradient_accumulation_steps: 64
  shuffle_buffer_size: 8192
  cache_dir: "/workspace/cache"

optimizer:
  algorithm: adam_w
  learning_rate: 1.5e-4
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  epsilon: 1.0e-8

scheduler:
  strategy: cosine_with_warmup
  warmup_steps: 2000
  total_steps: 400000
  total_epochs: null
  min_lr: 1.0e-5
  max_lr: null

runtime:
  seed: 42
  precision: bf16
  log_every_n_steps: 20
  checkpoint:
    directory: "/workspace/runs/runpod-2b/checkpoints"
    every_n_steps: 1000
    every_n_epochs: null
    max_keep: 5
  evaluation:
    every_n_steps: 5000
    every_n_epochs: null
    max_batches: 16
    best:
      directory: "/workspace/runs/runpod-2b/best"
      max_keep: 2
  logging:
    enable_stdout: true
    tensorboard: "/workspace/runs/runpod-2b/tensorboard"
    tensorboard_flush_every_n: 100
