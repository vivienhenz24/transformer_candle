# Experimental RunPod configuration targeting ~4B parameters.
# Single A100 80 GB is technically capable via large gradient accumulation,
# but throughput will be limited. Intended for future multi-GPU scale outs.

model:
  hidden_size: 3072
  intermediate_size: 12288
  num_layers: 32
  num_attention_heads: 48
  num_key_value_heads: 48
  max_position_embeddings: 4096
  attn_dropout: null
  residual_dropout: 0.0
  rope_mode: null
  rope_theta: null

tokenizer:
  tokenizer_json: "/workspace/artifacts/tokenizer/runpod-4b/tokenizer.json"
  vocab: "/workspace/artifacts/tokenizer/runpod-4b/vocab.json"
  merges: "/workspace/artifacts/tokenizer/runpod-4b/merges.txt"
  special_tokens: "/workspace/runs/runpod-4b/special_tokens.txt"

data:
  train_shards:
    - "/workspace/datasets/train/runpod-4b/train_0000.txt"
  validation_shards:
    - "/workspace/datasets/val/runpod-4b/val_0000.txt"
  batch_size: 1
  gradient_accumulation_steps: 256
  shuffle_buffer_size: 65536
  num_workers: 8
  cache_dir: "/workspace/cache/runpod-4b"

optimizer:
  algorithm: adam_w
  learning_rate: 1.5e-4
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  epsilon: 1.0e-8

scheduler:
  strategy: cosine_with_warmup
  warmup_steps: 6000
  total_steps: 400000
  total_epochs: null
  min_lr: 1.5e-5
  max_lr: null

runtime:
  seed: 42
  precision: bf16
  log_every_n_steps: 100
  checkpoint:
    directory: "/workspace/runs/runpod-4b/checkpoints"
    every_n_steps: 4000
    every_n_epochs: null
    max_keep: 10
  evaluation:
    every_n_steps: 8000
    every_n_epochs: null
    max_batches: 32
    best:
      directory: "/workspace/runs/runpod-4b/best"
      max_keep: 2
  logging:
    enable_stdout: true
    tensorboard: "/workspace/runs/runpod-4b/tensorboard"
    tensorboard_flush_every_n: 200
