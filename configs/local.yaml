# Example training configuration stored in YAML. Adjust paths and hyperparameters as needed.
model:
  hidden_size: 128
  intermediate_size: 512
  num_layers: 2
  num_attention_heads: 2
  num_key_value_heads: 2
  max_position_embeddings: 256

  # Optional overrides
  attn_dropout: null
  residual_dropout: null
  rope_mode: null
  rope_theta: null

# Tokenizer artifacts produced by the tokenizer crate or orchestrator CLI
# Update the paths to match your environment.
tokenizer:
  tokenizer_json: "runs/local-smoke/tokenizer/tokenizer.json"
  vocab: null
  merges: null
  special_tokens: "runs/local-smoke/special_tokens.txt"

data:
  train_shards:
    - "crates/pretraining-data/input.txt"
  validation_shards:
    - "crates/pretraining-data/input.txt"
  batch_size: 4
  gradient_accumulation_steps: 1
  shuffle_buffer_size: 2048
  cache_dir: null

optimizer:
  algorithm: adam_w
  learning_rate: 0.0005
  weight_decay: 0.0
  beta1: 0.9
  beta2: 0.95
  epsilon: 1.0e-8

scheduler:
  strategy: constant
  warmup_steps: 20
  total_steps: 200
  total_epochs: null
  min_lr: null
  max_lr: null

runtime:
  seed: 42
  precision: fp32
  log_every_n_steps: 1
  checkpoint:
    directory: "runs/local-smoke/checkpoints"
    every_n_steps: 50
    every_n_epochs: null
    max_keep: 2
  evaluation:
    every_n_steps: 50
    every_n_epochs: null
    max_batches: 1
    best:
      directory: "runs/local-smoke/best"
      max_keep: 1
  logging:
    enable_stdout: true
    tensorboard: "runs/local-smoke/tensorboard"
    tensorboard_flush_every_n: 10
